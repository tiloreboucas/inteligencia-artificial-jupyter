{"cells":[{"cell_type":"markdown","metadata":{"id":"CInd-FXyVoZC"},"source":["# Projeto 03 - Converse com documentos usando RAG avan√ßada\n","\n","> Nesse projeto iremos aprender a como criar uma pipeline de RAG mais avan√ßada para que seja capaz de:\n","* fazer perguntas a algum documento lido, como se fosse um chat com o pr√≥prio arquivo.\n","* consultar mais de uma refer√™ncia ao mesmo tempo.\n","* entender o contexto das mensagens passadas, usando o hist√≥rico da conversa tamb√©m como uma refer√™ncia para formular a resposta\n","\n","E para essa aplica√ß√£o tamb√©m ser√° constru√≠da uma interface.\n","Portanto, podemos reaproveitar parte do c√≥digo do projeto anterior e assim ir adicionando as novas funcionalidades"]},{"cell_type":"markdown","metadata":{"id":"2Khq09Wegw2H"},"source":["## [ ! ] Como executar em ambiente local\n","Para executar o c√≥digo desse projeto em um ambiente local, siga as instru√ß√µes para instalar as depend√™ncias necess√°rias usando os comandos abaixo. Voc√™ pode usar os mesmos comandos de instala√ß√£o.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EqT4YzGtWZGq"},"source":["## Instala√ß√£o e Configura√ß√£o\n","\n","Aqui iremos carregar primeiramente todos as fun√ß√µes que usamos no projeto anterior e mais algumas outras. Entra elas, o FAISS (um vectorstore no mesmo estilo do Chroma, que usamos nas aulas anteriores sobre RAG) e tamb√©m outras fun√ß√µes necess√°rias para implementa√ß√£o de uma pipeline RAG que entenda o contexto das conversas.\n","\n","Lembrando: podemos reaproveitar parte do c√≥digo que criamos no projeto 02.\n","Ent√£o se quiser pode fazer uma c√≥pia e fazer as modifica√ß√µes a partir dele.\n","\n","Abaixo, cada mudan√ßa que ser√° feita a partir desse arquivo, assim poder√° ir acompanhando as altera√ß√µes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25827,"status":"ok","timestamp":1734211838964,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"OEG8KS746Wmu","outputId":"5b6a17b9-fba9-4973-ba04-b6f9a96f7cb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q streamlit langchain\n","!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"]},{"cell_type":"markdown","metadata":{"id":"tv4u-p4J0l9K"},"source":["### Instala√ß√£o do FAISS\n","\n","Antes de importar √© necess√°rio que instalemos o FAISS, ele n√£o vem instalado por padr√£o no Colab. Portanto, podemos usar aqui e em ambiente local esse mesmo comando para instalar:\n","\n","`pip install -q faiss-cpu`\n","\n","voc√™ tamb√©m pode instalar `faiss-gpu` caso queira usar a vers√£o otimizada para GPU. Para simplificar no momento, iremos usar a vers√£o padr√£o por CPU mesmo\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8739,"status":"ok","timestamp":1734211847693,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"j-hJoWdJEBmU","outputId":"d8e4bf21-36b0-474d-8de4-de379209e55c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q faiss-cpu"]},{"cell_type":"markdown","metadata":{"id":"paEkaqQI0o7V"},"source":["Em seguida, use em sua aplica√ß√£o o `import faiss` e tamb√©m importar o `FAISS` dentro de biblioteca langchain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MP19iHfeEETw"},"outputs":[],"source":["import faiss\n","from langchain_community.vectorstores import FAISS"]},{"cell_type":"markdown","metadata":{"id":"ULIGaJSU0qca"},"source":["### Instala√ß√£o do PyPDFLoader\n","\n","Usaremos o PyPDFLoader para fazer a leitura dos arquivos PDF em nossa aplica√ß√£o. Isso ser√° explicado com detalhes na devida se√ß√£o.\n","E para podermos us√°-lo, precisamos antes instalar a biblioteca com o comando abaixo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4526,"status":"ok","timestamp":1734211854113,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"jf2iNG4hEGo-","outputId":"8d1ea74f-d79c-41ac-bc20-034b16dfe214"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pypdf\n","  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n","Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n","\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m297.0/298.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","Successfully installed pypdf-5.1.0\n"]}],"source":["!pip install pypdf"]},{"cell_type":"markdown","metadata":{"id":"yUTV--pa0tHW"},"source":["### Demais instala√ß√µes\n","\n","Assim como no projeto anterior, vamos instalar aqui o dotenv de novo (em ambiente local n√£o precisa executar a instala√ß√£o de novo, mas aqui no Colab como √© uma nova sess√£o precisamos) e tamb√©m o localtunnel (lembrando que esse n√£o √© necess√°rio em ambiente local)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10529,"status":"ok","timestamp":1734211864634,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"xo38vjjlEJb6","outputId":"ef168b47-7434-4f0d-8d76-37694eb8c8ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K\n","added 22 packages in 4s\n","\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K\n","\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K3 packages are looking for funding\n","\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K  run `npm fund` for details\n","\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K"]}],"source":["!pip install -q python-dotenv\n","!npm install -q localtunnel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1734211864634,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"uHxWcGDqEQZ0","outputId":"5670b28e-5566-4280-9cf3-e859af1d894e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing .env\n"]}],"source":["%%writefile .env\n","HUGGINGFACE_API_KEY=##########\n","HUGGINGFACEHUB_API_TOKEN=##########\n","OPENAI_API_KEY=##########\n","TAVILY_API_KEY=##########\n","SERPAPI_API_KEY=##########\n","LANGCHAIN_API_KEY=##########"]},{"cell_type":"markdown","metadata":{"id":"gmGl3IJyUB0U"},"source":["---\n","\n","## Inicializa√ß√£o da interface\n","\n","Por fim, reunimos todo o c√≥digo em um √∫nico script e adicionamos a configura√ß√£o da p√°gina com st.set_page_config e st.title, alterando o t√≠tulo e o emoji para deixar a interface mais personalizada e adequada ao projeto atual, com um visual mais alinhado com o contexto desse projeto"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1734211864635,"user":{"displayName":"Gabriel Alves","userId":"07503873055563742982"},"user_tz":180},"id":"0P5CTavqvh74","outputId":"73013970-0efc-438c-f5f9-9c68b7219e07"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing projeto3.py\n"]}],"source":["%%writefile projeto3.py\n","\n","import streamlit as st\n","from langchain_core.messages import AIMessage, HumanMessage\n","from langchain_core.prompts import MessagesPlaceholder\n","\n","from langchain_ollama import ChatOllama\n","from langchain_openai import ChatOpenAI\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n","\n","import torch\n","from langchain_huggingface import ChatHuggingFace\n","from langchain_community.llms import HuggingFaceHub\n","\n","import faiss\n","import tempfile\n","import os\n","import time\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","# Configura√ß√µes do Streamlit\n","st.set_page_config(page_title=\"Converse com documentos üìö\", page_icon=\"üìö\")\n","st.title(\"Converse com documentos üìö\")\n","\n","model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n","\n","## Provedores de modelos\n","def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n","  llm = HuggingFaceHub(\n","      repo_id=model,\n","      model_kwargs={\n","          \"temperature\": temperature,\n","          \"return_full_text\": False,\n","          \"max_new_tokens\": 512,\n","          #\"stop\": [\"<|eot_id|>\"],\n","          # demais par√¢metros que desejar\n","      }\n","  )\n","  return llm\n","\n","def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n","    llm = ChatOpenAI(\n","        model=model,\n","        temperature=temperature\n","        # demais par√¢metros que desejar\n","    )\n","    return llm\n","\n","def model_ollama(model=\"phi3\", temperature=0.1):\n","    llm = ChatOllama(\n","        model=model,\n","        temperature=temperature,\n","    )\n","    return llm\n","\n","\n","## Indexa√ß√£o e Recupera√ß√£o\n","\n","def config_retriever(uploads):\n","    # Carregar documentos\n","    docs = []\n","    temp_dir = tempfile.TemporaryDirectory()\n","    for file in uploads:\n","        temp_filepath = os.path.join(temp_dir.name, file.name)\n","        with open(temp_filepath, \"wb\") as f:\n","            f.write(file.getvalue())\n","        loader = PyPDFLoader(temp_filepath)\n","        docs.extend(loader.load())\n","\n","    # Divis√£o em peda√ßos de texto / Split\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=1000,\n","        chunk_overlap=200\n","    )\n","    splits = text_splitter.split_documents(docs)\n","\n","    # Embeddings\n","    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n","\n","    # Armazenamento\n","    vectorstore = FAISS.from_documents(splits, embeddings)\n","\n","    vectorstore.save_local('vectorstore/db_faiss')\n","\n","    # Configurando o recuperador de texto / Retriever\n","    retriever = vectorstore.as_retriever(\n","        search_type='mmr',\n","        search_kwargs={'k':3, 'fetch_k':4}\n","    )\n","\n","    return retriever\n","\n","\n","def config_rag_chain(model_class, retriever):\n","\n","    ### Carregamento da LLM\n","    if model_class == \"hf_hub\":\n","        llm = model_hf_hub()\n","    elif model_class == \"openai\":\n","        llm = model_openai()\n","    elif model_class == \"ollama\":\n","        llm = model_ollama()\n","\n","    # Para defini√ß√£o dos prompts\n","    if model_class.startswith(\"hf\"):\n","        token_s, token_e = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\", \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n","    else:\n","        token_s, token_e = \"\", \"\"\n","\n","    # Prompt de contextualiza√ß√£o\n","    context_q_system_prompt = \"Given the following chat history and the follow-up question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n","\n","    context_q_system_prompt = token_s + context_q_system_prompt\n","    context_q_user_prompt = \"Question: {input}\" + token_e\n","    context_q_prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", context_q_system_prompt),\n","            MessagesPlaceholder(\"chat_history\"),\n","            (\"human\", context_q_user_prompt),\n","        ]\n","    )\n","\n","    # Chain para contextualiza√ß√£o\n","    history_aware_retriever = create_history_aware_retriever(\n","        llm=llm, retriever=retriever, prompt=context_q_prompt\n","    )\n","\n","    # Prompt para perguntas e respostas (Q&A)\n","    qa_prompt_template = \"\"\"Voc√™ √© um assistente virtual prestativo e est√° respondendo perguntas gerais.\n","    Use os seguintes peda√ßos de contexto recuperado para responder √† pergunta.\n","    Se voc√™ n√£o sabe a resposta, apenas diga que n√£o sabe. Mantenha a resposta concisa.\n","    Responda em portugu√™s. \\n\\n\n","    Pergunta: {input} \\n\n","    Contexto: {context}\"\"\"\n","\n","    qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)\n","\n","    # Configurar LLM e Chain para perguntas e respostas (Q&A)\n","\n","    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n","\n","    rag_chain = create_retrieval_chain(\n","        history_aware_retriever,\n","        qa_chain,\n","    )\n","\n","    return rag_chain\n","\n","\n","## Cria painel lateral na interface\n","uploads = st.sidebar.file_uploader(\n","    label=\"Enviar arquivos\", type=[\"pdf\"],\n","    accept_multiple_files=True\n",")\n","if not uploads:\n","    st.info(\"Por favor, envie algum arquivo para continuar!\")\n","    st.stop()\n","\n","\n","if \"chat_history\" not in st.session_state:\n","    st.session_state.chat_history = [\n","        AIMessage(content=\"Ol√°, sou o seu assistente virtual! Como posso ajudar voc√™?\"),\n","    ]\n","\n","if \"docs_list\" not in st.session_state:\n","    st.session_state.docs_list = None\n","\n","if \"retriever\" not in st.session_state:\n","    st.session_state.retriever = None\n","\n","for message in st.session_state.chat_history:\n","    if isinstance(message, AIMessage):\n","        with st.chat_message(\"AI\"):\n","            st.write(message.content)\n","    elif isinstance(message, HumanMessage):\n","        with st.chat_message(\"Human\"):\n","            st.write(message.content)\n","\n","# para gravar quanto tempo levou para a gera√ß√£o\n","start = time.time()\n","user_query = st.chat_input(\"Digite sua mensagem aqui...\")\n","\n","if user_query is not None and user_query != \"\" and uploads is not None:\n","\n","    st.session_state.chat_history.append(HumanMessage(content=user_query))\n","\n","    with st.chat_message(\"Human\"):\n","        st.markdown(user_query)\n","\n","    with st.chat_message(\"AI\"):\n","\n","        if st.session_state.docs_list != uploads:\n","            print(uploads)\n","            st.session_state.docs_list = uploads\n","            st.session_state.retriever = config_retriever(uploads)\n","\n","        rag_chain = config_rag_chain(model_class, st.session_state.retriever)\n","\n","        result = rag_chain.invoke({\"input\": user_query, \"chat_history\": st.session_state.chat_history})\n","\n","        resp = result['answer']\n","        st.write(resp)\n","\n","        # mostrar a fonte\n","        sources = result['context']\n","        for idx, doc in enumerate(sources):\n","            source = doc.metadata['source']\n","            file = os.path.basename(source)\n","            page = doc.metadata.get('page', 'P√°gina n√£o especificada')\n","\n","            ref = f\":link: Fonte {idx}: *{file} - p. {page}*\"\n","            print(ref)\n","            with st.popover(ref):\n","                st.caption(doc.page_content)\n","\n","    st.session_state.chat_history.append(AIMessage(content=resp))\n","\n","end = time.time()\n","print(\"Tempo: \", end - start)"]},{"cell_type":"markdown","metadata":{"id":"M54-PNNEPrxh"},"source":["### Execu√ß√£o do Streamlit\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrnuMrbds4Na"},"outputs":[],"source":["!streamlit run projeto3.py &>/content/logs.txt &"]},{"cell_type":"markdown","metadata":{"id":"yYiheBeuN8SA"},"source":["E agora para nos conectar, usamos o comando abaixo (explica√ß√µes no colab do projeto 02)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"IMjgsW8c1NvZ","outputId":"e17beb7c-83fe-400c-b102-5e10b36a5602"},"outputs":[{"name":"stdout","output_type":"stream","text":["34.16.159.1\n","\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0Kyour url is: https://short-lizards-hunt.loca.lt\n"]}],"source":["!wget -q -O - ipv4.icanhazip.com\n","\n","!npx localtunnel --port 8501"]},{"cell_type":"markdown","metadata":{"id":"Ex1IJrK34UCw"},"source":["## Como melhorar üöÄ\n","\n","Sabendo exatamente como cada fun√ß√£o opera e entendendo as explica√ß√µes tratadas ao longo deste projeto, voc√™ tem o conhecimento necess√°rio para melhorar os resultados da sua aplica√ß√£o RAG. Vamos listar abaixo as estrat√©gias que podem ser aplicadas para otimizar a qualidade das respostas e a efici√™ncia do sistema:\n","\n","* Testar outros modelos de Embedding - conforme citado, a sele√ß√£o do modelo correto para o sistema RAG √© crucial, pois afeta diretamente a precis√£o das respostas, al√©m da utiliza√ß√£o de recursos e a escalabilidade da aplica√ß√£o. Escolher modelos que funcionem bem com o idioma e a tarefa em quest√£o pode melhorar significativamente os resultados. Ao testar diferentes modelos, voc√™ pode identificar qual oferece a melhor combina√ß√£o de qualidade e efici√™ncia para as suas necessidades.\n","\n","* Ajustar o prompt fixo (do sistema) - Modificar o prompt do sistema para torn√°-lo mais expl√≠cito sobre as fun√ß√µes que a LLM deve desempenhar pode melhorar os resultados. O prompt deve especificar com clareza o que a LLM deve priorizar na resposta e o que deve ser ignorado. Isso orienta o modelo a focar no que √© mais relevante para sua aplica√ß√£o e seu objetivo.\n","\n","* Melhorar o prompt do usu√°rio - lembrar o usu√°rio (colocando um aviso na interface talvez) que quanto mais espec√≠fico for na pergunta maior a chance de aumentar a precis√£o das respostas geradas pela LLM. Quanto mais detalhado e claro o pedido, mais relevante ser√° o retorno. Esta pr√°tica tamb√©m ajuda a reduzir ambiguidades que podem prejudicar a interpreta√ß√£o da consulta pelo modelo.\n","\n","* Ajustar o prompt de contextualiza√ß√£o - lembrando que este prompt reformula a pergunta do usu√°rio com base no hist√≥rico da conversa, algo √∫til quando a consulta precisa de contexto para ser corretamente interpretada. O prompt de contextualiza√ß√£o (context_q_system_prompt) instrui o modelo a levar o hist√≥rico em considera√ß√£o; e embora o prompt atual esteja em ingl√™s devido √† maior chance de compatibilidade da LLM com este idioma (apesar de ser compat√≠vel com o nosso), voc√™ pode test√°-lo em portugu√™s e assim fica f√°cil modificar o texto para maximizar o desempenho no idioma desejado.\n","\n","* Testar outras LLMs - Explorar outros modelos de linguagem, especialmente aqueles que aceitam uma quantidade maior de tokens e t√™m bom desempenho no idioma escolhido, pode melhorar a performance. Para casos mais exigentes, pode valer a pena considerar solu√ß√µes propriet√°rias como o ChatGPT ou servi√ßos pagos (como o Groq, citado no Colab 1) que disponibilizam grandes modelos de c√≥digo aberto. Modelos maiores podem lidar melhor com consultas complexas e fornecer respostas mais elaboradas.\n","\n","* Ajustar os par√¢metros de recupera√ß√£o (k e fetch_k) - Modificar os par√¢metros das etapas de recupera√ß√£o, como os valores de k e fetch_k, pode ter um impacto significativo no desempenho da sua aplica√ß√£o. Experimente come√ßar com valores menores e aument√°-los conforme necess√°rio, sempre monitorando o impacto na relev√¢ncia e qualidade das respostas. Para mais detalhes, consulte a se√ß√£o da pipeline RAG e o retriever. Outra ideia seria testar outros algoritmos al√©m do MMR.\n","\n","* Deixar melhor preparado para aceitar qualquer documento - uma ideia √© fazer o preprocessamento de arquivos PDF (ou outros formatos) para adequa√ß√£o ao vector store. Muitas vezes PDFs possuem tabelas ou outras estruturas que dificultam a interpreta√ß√£o; ou ainda, documentos em formatos mais diferentes como HTML, CSV, ou PPTX n√£o est√£o estruturados para extra√ß√£o ideal de informa√ß√µes. A prepara√ß√£o desses arquivos √© crucial para garantir que o conte√∫do relevante seja corretamente capturado e disponibilizado para o sistema de recupera√ß√£o.\n"," * Existem solu√ß√µes especializadas automatizam essa transforma√ß√£o, organizando os dados e eliminando informa√ß√µes desnecess√°rias. Isso otimiza o fluxo de trabalho e melhora a precis√£o dos resultados. Um exemplo √© o servi√ßo Unstructured (Acesse https://unstructured.io), que facilita a extra√ß√£o de dados complexos de arquivos, tornando-os prontos para uso em bancos de dados vetoriais e frameworks de LLMs, o que aumenta a qualidade da recupera√ß√£o da informa√ß√£o e o desempenho da aplica√ß√£o RAG.\n"," * Para usar isso no langchain √© simples, voc√™ pode usar o m√©todo de Document Loader. Na pr√°tica, basta carregar o documento usando o document loader Unstructured (ao inv√©s do PyPDFLoader que usamos). Mais detalhes aqui: https://python.langchain.com/v0.2/docs/integrations/document_loaders/unstructured_file/\n","\n","\n","Essas estrat√©gias visam otimizar a efici√™ncia e a qualidade das respostas do sistema RAG, adaptando-o ao seu caso de uso espec√≠fico.\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1ffUdoTetTjHOx3HIGi-0nvvg31e9cVYU","timestamp":1724761843583}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}