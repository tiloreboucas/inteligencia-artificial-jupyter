{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/dev/anaconda3/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: einops in /home/dev/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: accelerate in /home/dev/anaconda3/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: bitsandbytes in /home/dev/anaconda3/lib/python3.12/site-packages (0.45.0)\n",
      "Requirement already satisfied: filelock in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/dev/anaconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: networkx in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Baixando e Instalando os modelos e pacotes necessários\n",
    "%pip install transformers einops accelerate bitsandbytes\n",
    "\n",
    "# Transformers: Processamento de linguagem natural, classificação, tradução e geraão de texto\n",
    "\n",
    "# Einops: Manipulação de tensores (arrays multidimensionais)\n",
    "\n",
    "# Accelerate: Treinamento distribuído e paralelo, reduz a complexidade do treinamento\n",
    "\n",
    "# Bitsandbytes: Manipulação de bits e bytes, manupulação de arquivos binários e serialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoModelForCausalLM: Modelo de linguagem causal, geração de texto\n",
    "# AutoTokenizer: Tokenizador de texto, transforma texto em tokens\n",
    "# pipeline: Processamento de texto, tokenização, geração de texto, tradução, classificação, etc\n",
    "# BitsAndBytesConfig: Configuração para manipulação de bits e bytes\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch: Biblioteca de aprendizado de máquina, manipulação de tensores, cálculos matemáticos\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x755ddcd97910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuração do modelo e tokenizador estatico para geração de texto\n",
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/dev/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Guardar token\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libera memoria da GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()\n",
    "# torch.cuda.reset_max_memory_allocated() - obsoleto\n",
    "# torch.cuda.reset_max_memory_cached() - obsoleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_modelo = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/dev/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/dev/anaconda3/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0a50ccc67040e0a70304083b13e184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "\n",
    "# Definindo a variável de ambiente para evitar fragmentação \n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Desabilita o paralelismo na biblioteca tokenizers para evitar problemas de deadlock\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)  # Ativa a quantização para 8 bits\n",
    "\n",
    "\n",
    "# criando o modelo\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    id_modelo,\n",
    "    # device_map = 'balanced', # Usando a GPU/CPU automaticamente \n",
    "    device_map = 'cuda', # Usando a GPU\n",
    "    # torch_dtype = 'auto', # Ajusta automaticamente os tipos de dados dos tensores do modelo\n",
    "    torch_dtype = torch.float16, # Tipo de dado do modelo\n",
    "    trust_remote_code = True, # Permite o carregamento personalizado de modelos\n",
    "    # attn_implementation=\"eager\", # metodo de implementação pro metodo de atenção\n",
    "    quantization_config=quantization_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    id_modelo, \n",
    "    trust_remote_code = True # Permite o carregamento personalizado de modelos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# criação do pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model = model, \n",
    "    tokenizer = tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros de geração de texto\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500, # comprimento do texto de entrada\n",
    "    \"return_full_text\": False, # determina se retorna todo o texto ou apenas o que foi gerado\n",
    "    \"temperature\": 0.1, # controla a aleatoriedade da geração de texto\n",
    "    \"do_sample\": True, # determina se a aleatoriedade é usada na geração de texto\n",
    "    \"num_beams\": 3, # Número de feixes para busca \n",
    "    \"early_stopping\": True, # Pare a geração assim que uma sentença completa é gerada\n",
    "    \"truncation\": True # Ativar a truncação explicitamente\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explique o que é computação quântica\"\n",
    "output = pipe(prompt, **generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' e como ela difere da computação clássica.\\n\\nComputação quântica é um campo da ciência da computação que utiliza os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits binários (0 ou 1) para representar dados, a computação quântica usa qubits, que podem existir em um estado de 0, 1 ou ambos (superposição). Isso permite que os computadores quânticos realizem muitas operações simultaneamente, potencialmente acelerando significativamente certos tipos de cálculos.\\n\\nUma das principais diferenças entre computação quântica e clássica é o conceito de entrelaçamento quântico. Quando dois qubits estão entrelaçados, o estado de um qubit depende do estado do outro, independentemente da distância entre eles. Isso permite que os computadores quânticos realizem cálculos complexos de forma mais eficiente do que os computadores clássicos.\\n\\nOutra diferença chave é o uso de portas lógicas quânticas, que manipulam os estados quânticos dos qubits. Essas portas são diferentes das portas lógicas clássicas, pois podem realizar operações mais complexas, como criptografia quântica e teleportação quântica.\\n\\nEmbora a computação quântica tenha o potencial de revolucionar áreas como criptografia, simulação de materiais e otimização de problemas complexos, ainda está em estágios iniciais de desenvolvimento. Os computadores quânticos atuais são sensíveis a perturbações ambientais e têm um número limitado de qubits, o que limita sua capacidade de realizar cálculos em grande escala. No entanto, pesquisadores e empresas estão trabalhando ativamente para superar esses desafios e desenvolver computadores quânticos mais poderosos e estáveis.\\n\\nEm resumo, a computação quântica difere da computação clássica'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " e como ela difere da computação clássica.\n",
      "\n",
      "Computação quântica é um campo da ciência da computação que utiliza os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits binários (0 ou 1) para representar dados, a computação quântica usa qubits, que podem existir em um estado de 0, 1 ou ambos (superposição). Isso permite que os computadores quânticos realizem muitas operações simultaneamente, potencialmente acelerando significativamente certos tipos de cálculos.\n",
      "\n",
      "Uma das principais diferenças entre computação quântica e clássica é o conceito de entrelaçamento quântico. Quando dois qubits estão entrelaçados, o estado de um qubit depende do estado do outro, independentemente da distância entre eles. Isso permite que os computadores quânticos realizem cálculos complexos de forma mais eficiente do que os computadores clássicos.\n",
      "\n",
      "Outra diferença chave é o uso de portas lógicas quânticas, que manipulam os estados quânticos dos qubits. Essas portas são diferentes das portas lógicas clássicas, pois podem realizar operações mais complexas, como criptografia quântica e teleportação quântica.\n",
      "\n",
      "Embora a computação quântica tenha o potencial de revolucionar áreas como criptografia, simulação de materiais e otimização de problemas complexos, ainda está em estágios iniciais de desenvolvimento. Os computadores quânticos atuais são sensíveis a perturbações ambientais e têm um número limitado de qubits, o que limita sua capacidade de realizar cálculos em grande escala. No entanto, pesquisadores e empresas estão trabalhando ativamente para superar esses desafios e desenvolver computadores quânticos mais poderosos e estáveis.\n",
      "\n",
      "Em resumo, a computação quântica difere da computação clássica\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3?\n",
      "\n",
      "# Answer:Para resolver a expressão 7 + 6 / 2 - 3, precisamos seguir a ordem das operações, que é Parênteses, Expoentes, Multiplicação e Divisão (da esquerda para a direita), Adição e Subtração (da esquerda para a direita), também conhecida como PEMDAS.\n",
      "\n",
      "1. Primeiro, realizamos a divisão: 6 / 2 = 3\n",
      "2. Em seguida, realizamos a adição: 7 + 3 = 10\n",
      "3. Por fim, realizamos a subtração: 10 - 3 = 7\n",
      "\n",
      "Portanto, a resposta é 7.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quanto é 7 + 6 / 2\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Quem foi a primeira pessoa a ir pro espaço?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "template = \"\"\"<|system|>\n",
    "You are a helpful assistant.<|end>\n",
    "\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nYou are a helpful assistant.<|end>\\n\\n<|user|>\\n\"Quanto é 7 + 6 / 2\"<|end|>\\n<|assistant|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A primeira pessoa a ir ao espaço foi Yuri Gagarin, um cosmonauta soviético. Ele completou uma órbita ao redor da Terra em 12 de abril de 1961, a bordo da nave espacial Vostok 1, tornando-se um ícone mundial da exploração espacial.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Você é um assitente virtual que sempre responde em português do Brasil<|end|>\n",
      "<|user|>\n",
      "\"Qual é a capital do Brasil?\"<|end|>\n",
      "<|assistant|>\n",
      " A capital do Brasil é Brasília.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Qual é a capital do Brasil?\" # @param {type: string}\n",
    "\n",
    "sys_prompt = \"Você é um assitente virtual que sempre responde em português do Brasil\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "{}<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
    "\n",
    "output = pipe(template, **generation_args)\n",
    "\n",
    "print(template)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A IA, ou Inteligência Artificial, é um ramo da ciência da computação que se dedica ao desenvolvimento de sistemas capazes de realizar tarefas que normalmente requerem inteligência humana. Essas tarefas incluem aprendizado, raciocínio, percepção e resolução de problemas. A IA pode ser dividida em duas categorias principais: IA fraca, também conhecida como inteligência artificial estreita, que é projetada para realizar tarefas específicas, e IA forte, ou inteligência artificial geral, que é capaz de realizar qualquer tarefa intelectual que um ser humano possa fazer. Exemplos de IA incluem assistentes virtuais como Siri e Alexa, sistemas de reconhecimento de fala, veículos autônomos e algoritmos de aprendizado de máquina.\n"
     ]
    }
   ],
   "source": [
    "# formato de msg\n",
    "\n",
    "prompt = \"O que é IA?\"\n",
    "\n",
    "msg = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(msg, **generation_args)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Paris, França\n",
      "\n",
      "2. Roma, Itália\n",
      "\n",
      "3. Londres, Reino Unido\n",
      "\n",
      "4. Berlim, Alemanha\n",
      "\n",
      "5. Madrid, Espanha\n",
      "\n",
      "6. Lisboa, Portugal\n",
      "\n",
      "7. Viena, Áustria\n",
      "\n",
      "8. Amsterdã, Países Baixos\n",
      "\n",
      "9. Budapeste, Hungria\n",
      "\n",
      "10. Estocolmo, Suécia\n"
     ]
    }
   ],
   "source": [
    "# formato de msg\n",
    "\n",
    "prompt = \"Liste o nome de 10 cidades famosas da Europa\"\n",
    "\n",
    "msg = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(msg, **generation_args)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224b353a9ac04ef9848262c3c6c9bed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8113d7195c48e8bc3f5b46a31c692f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9aecc465544779b97e29c07f58c5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d571c8615e424629952067b75339157d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeceb0752e1b4a509b323197e05069a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd260271c8d4ac69a820453bbc5aae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1bdf95f1804ae98f82bd5982dc572f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ab3a61531042678fc0855dc92a716e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04259d054f04fa181b81e18b8052b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e06c6642b164a57addb64f747b179bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/75/15/75158e9667a787ec0ccbd8d18412c5d9a5f7326c543b803c4932dbc95951ed30/92ecfe1a2414458b4821ac8c13cf8cb70aed66b5eea8dc5ad9eeb4ff309d6d7b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00004-of-00004.safetensors%3B+filename%3D%22model-00004-of-00004.safetensors%22%3B&Expires=1736620164&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNjYyMDE2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzc1LzE1Lzc1MTU4ZTk2NjdhNzg3ZWMwY2NiZDhkMTg0MTJjNWQ5YTVmNzMyNmM1NDNiODAzYzQ5MzJkYmM5NTk1MWVkMzAvOTJlY2ZlMWEyNDE0NDU4YjQ4MjFhYzhjMTNjZjhjYjcwYWVkNjZiNWVlYThkYzVhZDllZWI0ZmYzMDlkNmQ3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=bJLboTgOVmb8fIb9Dohi707zK4DDVMc%7ElGYmjkLQZ67AslnKRHMaIgatvZS44xaK9nJxNmI%7EGufNHqmyo2cY06euRglwOjKOlATU0Zh2mktuXll32-wKOHMCdqyOI6tnGa11Z22Ve1TqEXE80P59bglwfS1zUAl-eulP9THsR9lVtyplMjDp-QIiNa5aXoPSoAtUE6x%7Emp4-yz1fXTt-22anDcEbmv1BI6ZpEWjJFmcFgVBsvZYuEGXCj1J90gSHakd-V8fKMDuAL6qv37ucJpzVc%7EtZAP90umk8qCS%7EzLNC2UFXK23MmYqcphFLpTy9tQEWZu1mZa-xWgTqJs9g1A__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede6cc276359488ab8488847d4b266b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:  17%|#7        | 199M/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.__init__() got an unexpected keyword argument 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# criando o tokenizador\u001b[39;00m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     14\u001b[0m     model_id, \n\u001b[1;32m     15\u001b[0m     trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Permite o carregamento personalizado de modelos\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     )\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     19\u001b[0m     model_id, \n\u001b[1;32m     20\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mquantization_config,\n\u001b[1;32m     21\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[1;32m     22\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4130\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4125\u001b[0m     special_dtypes\u001b[38;5;241m.\u001b[39mupdate(hf_quantizer\u001b[38;5;241m.\u001b[39mget_special_dtypes_update(model, torch_dtype))\n\u001b[1;32m   4127\u001b[0m special_dtypes\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   4128\u001b[0m     {\n\u001b[1;32m   4129\u001b[0m         name: torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m-> 4130\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()\n\u001b[1;32m   4131\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m keep_in_fp32_modules)\n\u001b[1;32m   4132\u001b[0m     }\n\u001b[1;32m   4133\u001b[0m )\n\u001b[1;32m   4135\u001b[0m target_dtype \u001b[38;5;241m=\u001b[39m torch_dtype\n\u001b[1;32m   4137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.__init__() got an unexpected keyword argument 'tokenizer'"
     ]
    }
   ],
   "source": [
    "# Configuração de Quantização para Otimização de Memória e Desempenho\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Habilita a quantização do modelo para 4 bits, reduzindo o uso de memória durante o carregamento.\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Define o tipo de quantização como 'nf4' (Normal Float 4), otimizando precisão e desempenho.\n",
    "    bnb_4bit_use_double_quant=True,  # Ativa a quantização dupla, aplicando uma segunda etapa de quantização para compactar ainda mais os pesos.\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Especifica o tipo de dado como float16 (meia precisão) para cálculos durante o uso do modelo, economizando memória e acelerando o processamento.\n",
    ")\n",
    "\n",
    "# model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# criando o tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code = True # Permite o carregamento personalizado de modelos\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=quantization_config,\n",
    "    tokenizer = tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Quem foi a primeira pessoa a ir pro espaço?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifica as mensagens em tensores compatíveis com o modelo\n",
    "encodeds = tokenizer(messages, return_tensors=\"pt\")\n",
    "\n",
    "# Move os dados codificados para o dispositivo (GPU ou CPU)\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "# Gera texto com base nos dados de entrada\n",
    "generated_ids = model.generate(\n",
    "    model_inputs,                      # Dados de entrada do modelo\n",
    "    max_new_tokens=1000,               # Número máximo de tokens a serem gerados\n",
    "    do_sample=True,                    # Ativa a amostragem para maior diversidade na geração\n",
    "    pad_token_id=tokenizer.eos_token_id # Define o token de preenchimento como o token de fim de sequência\n",
    ")\n",
    "\n",
    "# Decodifica os tokens gerados em texto legível\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "# Extrai o resultado decodificado\n",
    "res = decoded[0]\n",
    "res  # Retorna o texto gerado\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
