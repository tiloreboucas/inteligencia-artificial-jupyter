{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/dev/anaconda3/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: einops in /home/dev/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: accelerate in /home/dev/anaconda3/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: bitsandbytes in /home/dev/anaconda3/lib/python3.12/site-packages (0.45.0)\n",
      "Requirement already satisfied: filelock in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dev/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/dev/anaconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: networkx in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Baixando e Instalando os modelos e pacotes necessários\n",
    "%pip install transformers einops accelerate bitsandbytes\n",
    "\n",
    "# Transformers: Processamento de linguagem natural, classificação, tradução e geraão de texto\n",
    "\n",
    "# Einops: Manipulação de tensores (arrays multidimensionais)\n",
    "\n",
    "# Accelerate: Treinamento distribuído e paralelo, reduz a complexidade do treinamento\n",
    "\n",
    "# Bitsandbytes: Manipulação de bits e bytes, manupulação de arquivos binários e serialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoModelForCausalLM: Modelo de linguagem causal, geração de texto\n",
    "# AutoTokenizer: Tokenizador de texto, transforma texto em tokens\n",
    "# pipeline: Processamento de texto, tokenização, geração de texto, tradução, classificação, etc\n",
    "# BitsAndBytesConfig: Configuração para manipulação de bits e bytes\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch: Biblioteca de aprendizado de máquina, manipulação de tensores, cálculos matemáticos\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x77ce97f7f910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuração do modelo e tokenizador estatico para geração de texto\n",
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/dev/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Guardar token\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libera memoria da GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()\n",
    "# torch.cuda.reset_max_memory_allocated() - obsoleto\n",
    "# torch.cuda.reset_max_memory_cached() - obsoleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_modelo = \"microsoft/Phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/dev/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/dev/anaconda3/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/dev/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dev/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0a50ccc67040e0a70304083b13e184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "\n",
    "# Definindo a variável de ambiente para evitar fragmentação \n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Desabilita o paralelismo na biblioteca tokenizers para evitar problemas de deadlock\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)  # Ativa a quantização para 8 bits\n",
    "\n",
    "\n",
    "# criando o modelo\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    id_modelo,\n",
    "    # device_map = 'balanced', # Usando a GPU/CPU automaticamente \n",
    "    device_map = 'cuda', # Usando a GPU\n",
    "    # torch_dtype = 'auto', # Ajusta automaticamente os tipos de dados dos tensores do modelo\n",
    "    torch_dtype = torch.float16, # Tipo de dado do modelo\n",
    "    trust_remote_code = True, # Permite o carregamento personalizado de modelos\n",
    "    # attn_implementation=\"eager\", # metodo de implementação pro metodo de atenção\n",
    "    quantization_config=quantization_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    id_modelo, \n",
    "    trust_remote_code = True # Permite o carregamento personalizado de modelos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# criação do pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model = model, \n",
    "    tokenizer = tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros de geração de texto\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500, # comprimento do texto de entrada\n",
    "    \"return_full_text\": False, # determina se retorna todo o texto ou apenas o que foi gerado\n",
    "    \"temperature\": 0.1, # controla a aleatoriedade da geração de texto\n",
    "    \"do_sample\": True, # determina se a aleatoriedade é usada na geração de texto\n",
    "    \"num_beams\": 3, # Número de feixes para busca \n",
    "    \"early_stopping\": True, # Pare a geração assim que uma sentença completa é gerada\n",
    "    \"truncation\": True # Ativar a truncação explicitamente\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explique o que é computação quântica\"\n",
    "output = pipe(prompt, **generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' e como ela difere da computação clássica.\\n\\nComputação quântica é um campo da ciência da computação que utiliza os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits binários (0 ou 1) para representar dados, a computação quântica usa qubits, que podem existir em um estado de 0, 1 ou ambos (superposição). Isso permite que os computadores quânticos realizem muitas operações simultaneamente, potencialmente acelerando significativamente certos tipos de cálculos.\\n\\nUma das principais diferenças entre computação quântica e clássica é o conceito de entrelaçamento quântico. Quando dois qubits estão entrelaçados, o estado de um qubit depende do estado do outro, independentemente da distância entre eles. Isso permite que os computadores quânticos realizem cálculos complexos de forma mais eficiente do que os computadores clássicos.\\n\\nOutra diferença chave é o uso de portas lógicas quânticas, que manipulam os estados quânticos dos qubits. Essas portas são diferentes das portas lógicas clássicas, pois podem realizar operações mais complexas, como criptografia quântica e teleportação quântica.\\n\\nEmbora a computação quântica tenha o potencial de revolucionar áreas como criptografia, simulação de materiais e otimização de problemas complexos, ainda está em estágios iniciais de desenvolvimento. Os computadores quânticos atuais são sensíveis a perturbações ambientais e têm um número limitado de qubits, o que limita sua capacidade de realizar cálculos em grande escala. No entanto, pesquisadores e empresas estão trabalhando ativamente para superar esses desafios e desenvolver computadores quânticos mais poderosos e estáveis.\\n\\nEm resumo, a computação quântica difere da computação clássica'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " e como ela difere da computação clássica.\n",
      "\n",
      "Computação quântica é um campo da ciência da computação que utiliza os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits binários (0 ou 1) para representar dados, a computação quântica usa qubits, que podem existir em um estado de 0, 1 ou ambos (superposição). Isso permite que os computadores quânticos realizem muitas operações simultaneamente, potencialmente acelerando significativamente certos tipos de cálculos.\n",
      "\n",
      "Uma das principais diferenças entre computação quântica e clássica é o conceito de entrelaçamento quântico. Quando dois qubits estão entrelaçados, o estado de um qubit depende do estado do outro, independentemente da distância entre eles. Isso permite que os computadores quânticos realizem cálculos complexos de forma mais eficiente do que os computadores clássicos.\n",
      "\n",
      "Outra diferença chave é o uso de portas lógicas quânticas, que manipulam os estados quânticos dos qubits. Essas portas são diferentes das portas lógicas clássicas, pois podem realizar operações mais complexas, como criptografia quântica e teleportação quântica.\n",
      "\n",
      "Embora a computação quântica tenha o potencial de revolucionar áreas como criptografia, simulação de materiais e otimização de problemas complexos, ainda está em estágios iniciais de desenvolvimento. Os computadores quânticos atuais são sensíveis a perturbações ambientais e têm um número limitado de qubits, o que limita sua capacidade de realizar cálculos em grande escala. No entanto, pesquisadores e empresas estão trabalhando ativamente para superar esses desafios e desenvolver computadores quânticos mais poderosos e estáveis.\n",
      "\n",
      "Em resumo, a computação quântica difere da computação clássica\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3?\n",
      "\n",
      "# Answer:Para resolver a expressão 7 + 6 / 2 - 3, precisamos seguir a ordem das operações, que é Parênteses, Expoentes, Multiplicação e Divisão (da esquerda para a direita), Adição e Subtração (da esquerda para a direita), também conhecida como PEMDAS.\n",
      "\n",
      "1. Primeiro, realizamos a divisão: 6 / 2 = 3\n",
      "2. Em seguida, realizamos a adição: 7 + 3 = 10\n",
      "3. Por fim, realizamos a subtração: 10 - 3 = 7\n",
      "\n",
      "Portanto, a resposta é 7.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quanto é 7 + 6 / 2\"\n",
    "output = pipe(prompt, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Quem foi a primeira pessoa a ir pro espaço?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "template = \"\"\"<|system|>\n",
    "You are a helpful assistant.<|end>\n",
    "\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nYou are a helpful assistant.<|end>\\n\\n<|user|>\\n\"Quanto é 7 + 6 / 2\"<|end|>\\n<|assistant|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A primeira pessoa a ir ao espaço foi Yuri Gagarin, um cosmonauta soviético. Ele completou uma órbita ao redor da Terra em 12 de abril de 1961, a bordo da nave espacial Vostok 1, tornando-se um ícone mundial da exploração espacial.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A capital do Brasil é Brasília. Foi inaugurada em 21 de abril de 1960, substituindo a antiga capital, Rio de Janeiro. A cidade foi projetada pelo arquiteto Oscar Niemeyer e o urbanista Lúcio Costa, com o objetivo de promover o desenvolvimento econômico e político do interior do país.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Qual é a capital do Brasil?\" # @param {type: string}\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "\"{}\"<|end|>\n",
    "<|assistant|>\"\"\".format(prompt)\n",
    "\n",
    "output = pipe(template, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
